Plan


1)page de garde
2)sommaire
3)Intro
1.En quoi consiste notre projet?
2.Pourquoi on l'a choisi?
3.Quelle est la problématique?
4.A quoi ça sert?
4)Ce qu'on a utilisé ?
1.Les technologies
OPENCV
python
weka
java?
2.Les principes de fonctionnement
OPENCV : les méthodes utiles
python : avantages et inconvénients
weka : présentation des algorithmes, résultats, pourquoi?
3.Utilisation de ces technologies et pourquoi?
5)Le corpus de donnés
Qui?
Combien?
Répartition
Problèmes
6)Progression  de nos résultats
7)Index (définitions mots-clés)
8)Bibliographie

3) Introduction : 

L'objectif de notre TER est de reconnaitre l'expression des émotions sur les visages par ordinateur. 
Nous avons choisi ce projet pour différentes raisons. Premièrement, nous préférions travailler sur une base de données d'images, et pas sur des sons, ce qui nous a orienté vers les chiffres, les formes (objets), la reconnaissance faciale ou la reconnaissance d'émotions.
Il nous est apparu que ce projet était un bon compromis entre un sujet « simple » tel que la reconnaissance des chiffres et un sujet trop compliqué tel que la reconnaissance d'objets. Le projet de reconnaissance faciale étant déjà pris nous avons opté pour la détection des émotions sur des photos. Nous avons au cours du projet eu l'envie de l'adapter sur une webcam pour avoir plus d'interactivité.

On se servira de différents outils de vision par ordinateur et on se basera sur les travaux de Paul Ekman concernant la détection et classification des émotions.

5) Notre corpus : 

Classificateur présenté : weka.classifiers.trees.J48 -C 0.25 -M 2. D'autres méthodes (multilayer perceptron...) présentaient des résultats similaires (~ 33%).  
Correctly Classified Instances          26               35.1351 % 
Incorrectly Classified Instances        48               64.8649 % 
Kappa statistic                          0.2263 
Mean absolute error                      0.2041 
Root mean squared error                  0.3384 
Relative absolute error                 85.6635 % 
Root relative squared error             97.6585 %

Le score de 33% s'explique par la bonne détection de deux classes sur six au détriment des autres : la surprise et la joie. 

Nomenclature : 
AN : colère (Aucun échantillon)
DI : dégout (25)
FE : peur (32)
HA : joie (40)
NE : neutre (41)
SA : tristesse (38)
SU : Surprise (41)

4) Ce qu'on a utilisé

 - Weka
Arbre de décision J48 

Il représente les étapes prises pour décider d'une classification. L'arbre commence par un noeud racine, chaque n½ud évalue un critère, et détermine le n½ud suivant à évaluer. La classification est effectuée une fois une feuille atteinte. 

Les arbres de décision peuvent représenter divers types de donnée. Le plus simple étant des nombres, mais une énumération quelconque n'est pas contre indiqué. 

La construction s'effectue de manière récursive. D'abord, un attribut est sélectionné comme racine. Pour construire un arbre efficace, il est nécessaire que l'attribut choisi sépare les instances du problème de décision le plus équitablement possible. La meilleure partition est celle qui assure le meilleur gain d'information. Ensuite, un sous arbre est construit pour chaque sous partie. L'information, dans ce contexte provient de la théorie de l'information développée par Claude Shannon, le gain d'information est calculé à l'aide d'une formule appropriée, qui ne sera pas détaillée ici. 


L'algorithme utilisé par Weka est J48. C'est une variation du C4.5 de J. Ross Quinlan. Il semble important de comprendre la variété d'options d'élagage applicable à cet algorithme, car elles peuvent avoir un impact important sur la qualité des résultats. 

L'élagage d'arbre produit des résultats plus faciles à interpréter et peut éviter un sur-apprentissage, c'est à dire que la décision serait bonne sur l'ensemble d'apprentissage, et mauvaise en généralisation. Celui ci consiste en une substitution de sous-arbres, on peut remplacer un n½ud par une feuille ce qui réduit le nombre de tests dans cette branche de l'arbre. 

Les taux d'erreur sont utilisés pour décider de l'élagage optimal. Il y a plusieurs méthodes pour cela, la plus simple étant de réserver une parti du corpus pour tester l'arbre de décision que l'on a entrainé. Bien que cette méthode soit simple, elle présente l'inconvénient de diminuer la taille du corpus d'apprentissage. Elle est déconseillé pour de petits corpus. 


D'autres méthodes fondées sur le calcul de taux d'erreurs consistent à estimer l'erreur inhérente au corpus d'apprentissage. Cela permet de décider du degrés de généralité ou de spécificité du modèle. Si les données sont très fidèles au modèle, il n'est pas nécessaire d'élaguer pour éviter une erreur en généralisation. 

Certaines options déterminent la spécificité du modèle, comme le nombre minimum d'instances par feuilles. Plus ce nombre est élevé, plus l'arbre est général. L'option "binary split" est appliquée aux données numériques : elle limite le nombre de point de décision en n'autorisant qu'un partitionnement binaire d'une grandeur. 


Perceptron Multi-couches 

Le perceptron est un modèle mathématique inspiré des neurones inventé en 1957 par Frank Rosenblatt. Il s'agit du plus simple des réseaux de neurones artificielles : c'est un classificateur linéaire. Son principe de fonctionnement est simple : si le produit scalaire d'un vecteur d'entrée et d'un vecteur propre au "neurone" est supérieur à 0, la sortie du perceptron vaudra 1, et 0 sinon. Graphiquement, cela équivaut à classer un point au dessus ou au dessous d'un hyperplan, d'où le terme de classificateur linéaire. Une amélioration consiste à utiliser une autre fonction qu'un simple seuil pour déterminer la sortie. La fonction sigmoïde est recommandé, car elle est continue et présente une allure adaptée.

L'apprentissage est supervisé, et utilise un algorithme de propagation. L'erreur calculée sur la sortie est répercutée sur les poids du vecteur interne au "neurone". Dans le cas où les données d'apprentissage sont linéairement séparables, l'algorithme converge. Dans le cas contraire, il oscille. 

Afin de dépasser ces limitations inhérentes, on assemble plusieurs neurones afin de constituer un réseau, le perceptron multi-couches. Il s'agit d'un graphe orienté de neurones, réparti en plusieurs couches dont les entrée sont connectés aux sorties de la précédente. Un MLP de 2 couches permet d'approcher n'importe quelle fonction continue. Une troisième couche permet d'approximer une fonction arbitraire. 
